
\فصل{مفاهیم اولیه}



\قسمت{معماری شبکه}

همانطور که اشاره شد رشد فزاینده سیستم‌های هوشمند و نیازهای محاسباتی برنامه‌های کاربردی دیگر فقط از طریق منابع محاسباتی تجهزات کاربر امکان‌پذیر نخواهد بود، بنابرین محاسبات ابری نقش مهمی را در تامین منابع محاسباتی ایفا می‌کند. همچنین وجود نیازهای متنوع سیستم‌های هوشمند از جمله کاربردهای بی‌درنگ باعث غیر قابل تحمل بودن تأخیر شده‌است. از این رو انتقال محاسبات به ابر مرکزی در برخی موارد امکان‌پذیر نخواهد بود، محاسبات لبه با جایگیری در بخش میانی دستگاه‌های هوشمند و ابر مرکزی با نزدیک‌کردن منابع محاسباتی نقشی همانند محاسبات‌ابری اما به صورت توزیع‌شده و در فاصله نزدیک‌تر به دستگاه‌های هوشمند را ایفا می‌کنند. بنابراین می‌توان معماری سیستم را به شکل کلی شامل چند لایه متشکل از محاسبات‌ابری، محاسبات‌لبه و دستگاه‌های هوشمند همانطور که در شکل~\رجوع{شکل: معماری} نشان داده شده است، متصور بود.


\شروع{شکل}
\centerimg{Edge-Computing}{14cm}
\شرح{معماری شبکه با بهره‌گیری از محاسبات لبه}
\برچسب{شکل: معماری}
\پایان{شکل}

\زیرقسمت{محاسبات‌ ابری}

امروزه محاسبات ابری با فراهم کردن منابع محاسباتی قدرتمند به صورت متمرکز و در دسترس از راه دور نقش مهمی در تامین نیازهای محاسباتی سیستم‌های هوشمند را برعهده دارد. هدف اصلی محاسبات ابری میسر ساختن دسترسی به حجم عظیمی از منابع محاسباتی به صورت مجازی‌سازی‌شده است. این کار با استفاده از تجمیع منابع و ایجاد یک سیستم یکپارچه انجام میشود. خدمات ابری کاربران را قادر می سازند تا برنامه ها را در سرورهای راه دور ذخیره و اجرا کنند و سپس از طریق اینترنت به کلیه داده ها دسترسی پیدا کنند. این بدان معناست که کاربر برای دسترسی به اطلاعات محدودیت مکانی نخواهد داشت و از هر جایی از طریق اینترنت دسترسی خواهد داشت~\cite{gonzalez2015cloud}. سرورهای ابری از توان محاسباتی بالایی برخوردار هستند و با تجمیع اطلاعات دریافتی از نقاط محاسباتی دیگر از جمله دستگاه‌های هوشمند، سنسورها و سرورهای لبه می‌توانند سیستمی متمرکز و یکپارچه ایجاد کنند و این سیستم قادر خواهد بود در ارائه خدمات متنوع به دستگاه‌های هوشمند و بهبود عملکرد سیستم موثر واقع‌شود.


\زیرقسمت{محاسبات لبه متحرک}

در پی رشد دستگاه های هوشمند و برنامه های‌کاربردی، بهره‌گیری از محاسبات ابری توانسته تا حدی به توزیع بار محاسبات کمک کند و باعث بهبود عملکرد سیستم شود، اما با توجه به گسترش هر روزه این نیازهای محاسباتی، اجرای تمام برنامه‌ها در ابر مرکزی به راحتی منجر به ازدحام شبکه و تخریب عملکرد جدی می‌شود. همچنین وجود نیازهای متنوع برنامه‌ها از جمله کاربردهای بی درنگ، نمی‌توانند تأخیر ارسال\پاورقی{Transmission Delay} محاسبات به ابر را تحمل کنند، در خیلی از این مواقع این تأخیر می تواند سبب تخریب سیستم و حتی صدمات جدی باشد. به عنوان مثال تأخیر در واکنش یک وسیله نقلیه خودران\پاورقی{Autonomous Vehicles} و یا دستگاه‌های مورد استفاده در پزشکی، می‌تواند آسیب های جدی و غیرقابل جبرانی را در پی داشته‌باشد. در این زمان، به نظر می‌رسد که به عنوان یک مفهوم جدید از معماری شبکه و توسعه محاسبات ابری، محاسبات لبه یک راه حل امیدوارکننده برای پاسخگویی به این چالش باشد. 

محاسبات لبه با نزدیک‌کردن منابع محاسباتی و ذخیره‌سازی به کاربران انتهایی باعث کاهش تأخیر و بهبود عملکرد سیستم می‌شود و کیفیت تجربه مشتری را تا حد زیادی بهبود می‌بخشد. سرورهای لبه که بین دستگاه های هوشمند و مرکز داده ابری مستقر می شوند، خدمات محاسباتی، ذخیره سازی در شبکه را فراهم می کنند. محاسبات لبه همانند محاسبات ابری با فراهم کردن منابع محاسباتی در دسترس برای رفع نیازهای محاسباتی موجود در نظر گرفته می‌شود. 











تفاوت محاسبات لبه با محاسبات ابری را می‌توان در نحوه جایگیری منابع تعریف کرد. ساختار محاسبات ابری به شکلی متمرکز\پاورقی{Centralize} و در نقاط مکانی محدود تشکیل شده است اما محاسبات لبه سرورها را در نقاط مختلف و در نزدیکی دستگاه‌های هوشمند به شکلی توزیع‌شده\پاورقی{Distributed} استقرار می‌دهد. همچنین سرورهای لبه در مقایسه با ابر مرکزی از توان محاسباتی کمتری برخوردار هستند. سرورهای نزدیک به کاربر می‌توانند از دستگاه‌های هوشمند همراه برای ارتباطات بی‌وقفه پشتیبانی کنند. از ویژگی‌های متمایزکننده محاسبات لبه متحرک می‌توان به موارد زیر اشاره‌کرد:





\پاراگراف{پشتیبانی از تحرک کاربران}
همانطور که تعداد دستگاه‌های هوشمند متحرک به سرعت در حال افزایش است، محاسبات لبه متحرک با استفاده از روش‌های غیر متمرکز، از تحرک کاربران پشتیبانی می‌کند، بهره‌گیری از روش‌های توزیع‌شده، برای برقراری ارتباط مستقیم با دستگاه های متحرک، با جداسازی هویت میزبان از هویت محل، اصل کلیدی را در پشتیبانی تحرک در محاسبات لبه متحرک ایفا می‌کند.


\پاراگراف{توزیع جغرافیایی متراکم}

محاسبات لبه خدمات ابری را به کاربر نزدیک‌تر می‌کند و با استفاده از سیستم‌عامل‌های متعدد می‌تواند به توزیع جغرافیایی متراکم زیرساخت‌ها کمک‌کند، مدیران شبکه قادر خواهند بود خدمات متحرک مبتنی بر مکان را تسهیل‌کنند. همچنین تجزیه و تحلیل داده های بزرگ را می‌توان به سرعت با دقت بهتری انجام‌داد، سیستم‌های لبه، تجزیه و تحلیل بی‌درنگ را در مقیاس بزرگ ممکن می‌کنند.



\پاراگراف{آگاهی از موقعیت مکانی}
آگاهی از موقعیت مکانی کاربران در محاسبات لبه به کاربران متحرک این امکان را می‌دهد تا خدمات را از سرورهای لبه در نزدیک‌ترین مکان به موقعیت فیزیکی خود در دسترس داشته‌‌باشند. کاربران می‌توانند فناوری‌های مختلفی مانند زیرساخت‌های موقعیت‌یاب در تلفن همراه، و یا نقاط دسترسی بی‌سیم را برای اشتراک‌گذاری موقعیت مکانی خود به‌کار گیرند. این آگاهی از موقعیت مکانی می‌تواند در توزیع متناسب بار محاسباتی در شبکه و کاهش زمان انتقال محاسبات بسیار تاثیرگذار و مفید باشد.


\پاراگراف{نزدیکی به کاربران}
در محاسبات لبه متحرک، منابع محاسباتی و خدمات در نزدیکی کاربران در دسترس است و از این طریق کاربران می‌توانند تجربه خود را بهبود بخشند. در دسترس بودن منابع محاسباتی و خدمات در مجاورت محلی، کاربران را قادر می‌سازد اطلاعات مربوط به شبکه را برای تصمیم‌گیری‌تخلیه\پاورقی{Offloading Decision } و استفاده از خدمات، مورد استفاده قرار دهند. به طور مشابه، ارائه‌دهنده خدمات می‌تواند اطلاعات کاربران را استخراج‌کرده و با تجزیه و تحلیل رفتار کاربران در جهت بهبود خدمات و تخصیص منابع خود، استفاده‌کند.

\پاراگراف{زمان تأخیر کم}
پارادایم‌های محاسباتی لبه، منابع محاسباتی و خدمات را به کاربران نزدیک‌تر می‌کنند که به موجب آن تأخیر در دسترسی به خدمات را کاهش می‌دهد. تأخیر کم محاسبات لبه، کاربران را قادر می‌سازد تا محاسبات برنامه‌های کاربردی حساس به تأخیر خود را بر روی سرورهای لبه با منابع محاسباتی قدرتمند انجام‌دهند و تأخیر را کاهش‌دهند.

\پاراگراف{ناهمگونی }
ناهمگونی در محاسبات لبه به وجود سیستم‌عامل‌ها، ساختار، فرایند، فناوری، محاسبات و فن‌آوری‌های ارتباطی متنوع استفاده‌شده توسط عناصر محاسباتی لبه و دستگاه‌های پایانی و شبکه‌ها اشاره دارد. وجود چنین تفاوت‌هایی منجر به مسائل مربوط به قابلیت‌های عملیاتی شده‌است و آن را یک چالش اصلی در استقرار موفقیت‌آمیز محاسبات لبه ارائه می‌دهد. ناهمگونی شبکه به تنوع فن‌آوری‌های ارتباطی اشاره دارد که بر تحویل خدمات لبه تاثیر می‌گذارد.


هدف اصلی محاسبه لبه پیش‌پردازش داده‌ها، کاهش تأخیر و ارائه نیاز‌های برنامه‌های کاربردی با تأخیر کم و پاسخ در زمان کوتاه است~\cite{shi2016edge}. در حال حاضر، معماری رایج برای محاسبات لبه در ساختار یک شبکه چند لایه‌ی تشکیل‌شده از ابر مرکزی، سرورهای لبه و دستگاه‌های هوشمند انتهایی، همانطور که در شکل~\رجوع{شکل: معماری} نشان داده‌شده‌است، در نظر گرفته می‌شود.



%\زیرقسمت{محاسبات مه}

%محاسبه مه و محاسبات لبه ساختاری نسبتا مشابه را هدف قرار میدهند. در محاسبات مه همانند محاسبات لبه از منابع محاسباتی تحت یک ساختار توزیع شده به عنوان منابع محاسباتی مورد نیاز برای تامین نیازهای برنامه های کاربردی استفاده میشود. محاسبات مه، با استقرار سرورهای محلی در نقاط نزدیک به دستگاه های انتهایی، دادهها را از آن نقاط جمع آوری کرده و به یک دروازه9 منتقل میکند، که در آن جا سرور محلی قرار دارد. بنابرین یک محیط مه تشکیل شده از سرورهای توزیع شده میتواند در راستای تامین نیازهای برنامههای کاربردی از جمله نیازهای محاسباتی و همچنین نیازهای بیدرنگ در یک سیستم اینترنت اشیا10 بسیار موثر واقع شود~\cite{yi2015survey}. به نظر می رسد هم محاسبات لبه و هم محاسبات مه شامل نزدیک کردن منابع محاسباتی به محل درخواست ها و انجام محاسبات به شکل محلی باشند. با این حال، تفاوت هایی در این مفاهیم وجود دارد که در ادامه به تفکیک آن ها و معین کردن تقسیمبندی آنها می پردازیم.





\زیرقسمت{ابعاد محاسبات ابری و محاسبات لبه}

با توجه به ماهیت مشترک این مفاهیم در تامین نیازهای محاسباتی دستگاه‌های هوشمند و برنامه‌های کاربردی، در این بخش به تفکیک مشخصات و بررسی دامنه و ابعاد هر یک از این تعاریف می‌پردازیم. به طور کلی می‌توان تفاوت بین محاسبات ابری، محاسبات مه\پاورقی{Fog Computing} و محاسبات لبه را در محل محاسبات، توان محاسبات و هدف از محاسبات در نظر گرفت~\cite{dolui2017comparison}:

\پاراگراف{محل محاسبات}

تفاوت اصلی بین محاسبات ابری، محاسبات مه و محاسبات لبه، مکانی است که پردازش داده در آن انجام می‌شود. در محاسبات ابری، داده‌ها بر روی یک سرور ابری مرکزی پردازش می‌شوند، که معمولاً بسیار دور از منبع اطلاعات قرار دارند. در محاسبات لبه معمولا محاسبات مستقیماً روی دستگاه‌هایی که سنسورها به آنها متصل شده‌اند یا سرور‌های دروازه\پاورقی{Gateway}‌ شبکه که در مجاورت سنسورها قرار دارد، رخ می‌دهد، و از طرف دیگر، محاسبات مه وظایف محاسباتی لبه را به منابع محاسباتی که مستقیماً به سرورهای محلی متصل شده‌اند منتقل می‌کنند، که از نظر فیزیکی از عملگرها و سنسورها فاصله بیشتری دارند. 

بنابراین، برای محاسبات لبه، داده‌ها بر روی خود دستگاه‌ها و یا دستگاه نزدیک، پردازش می‌شوند بدون اینکه به جای دیگری منتقل شوند. و در محاسبات مه، داده‌ها درون یک دروازه که معمولا در شبکه محلی قرار دارد پردازش می‌شوند. اگرچه از نظر فاصله مکانی این دسته‌بندی معتبر خواهد‌بود اما این مفاهیم در هر معماری بخصوصی می‌تواند به شکل‌دیگری در نظر گرفته‌شود. به عنوان مثال یک سنسور که در درون بدن یک بیمار وظیفه ثبت اطلاعات را بر عهده دارد گوشی همراه در نزدیکی بیمار برای سنسور، گره لبه محسوب می‌شود. بنابراین این تعاریف با توجه به موقعیت و معماری مورد استفاده شکل دقیق‌تری به خواهد خواهد گرفت.


\پاراگراف{توان محاسبات}

توان محاسباتی منابع مورد استفاده در معماری محاسبات لبه متحرک با دورشدن از دستگاه‌های انتهایی بیشتر‌شده و با نزدیکی به آن‌ها کاهش می‌یابد. در بالاترین سطح و دورترین فاصله، محاسبات ابری قرار دارد که بیشترین توان پردازشی را دارا می‌باشد. در لایه‌ پایین‌تر محاسبات لبه که نسبت به محاسبات ابری از توان محاسباتی کمتری برخوردار هستند، قراردارند. به طور کلی می‌توان نتیجه‌گرفت با نزدیک‌شدن به دستگاه‌های هوشمند توان محاسباتی کاهش خواهد‌یافت، اما زمان پاسخ درخواست کمتر می‌شود و همچنین با دور شدن از نقاط انتهایی توان پردازشی افزایش‌یافته و زمان پاسخ نیز افزایش می‌یابد.


\پاراگراف{هدف محاسبات}

بعد دیگری از تفاوت‌های ساختاری محاسبات را می‌توان دلیل انجام آن محاسبات در نظر گرفت. به طور کلی محاسبات ابری برای تحلیل عمیق و طولانی‌مدت داده‌ها مناسب خواهد‌بود، و محاسبات لبه برای پردازش‌های کوتاه و تحلیل سریع و پاسخ در زمان کوتاه مناسب هستند. در اینجا ذکر این نکته نیز لازم است که محاسبات ابری به اتصال همیشگی اینترنت وابسته است، درحالی که سرورها در محاسبات لبه معمولا حتی بدون اتصال به اینترنت نیز می‌توانند فعالیت‌کنند. همچنین به لطف ساختار توزیع‌شده آن‌ها می‌توان محاسبات لبه را ایمن دانست. نسخه‌های مختلف از داده‌ها بین گره‌ها توزیع می‌شوند، و دستکاری داده‌ها در مقایسه با ساختار متمرکز محاسبات ابری بسیار دشوار خواهد‌بود. بنابراین، در مواردی که امنیت یک نگرانی اساسی است، محاسبات لبه ترجیح داده‌می‌شوند.

از آنجاکه داده‌ها در میان گره‌ها توزیع می‌شوند، خرابی یک گره باعث خرابی سیستم نخواهد‌شد، در نتیجه تحمل‌پذیری‌اشکال\پاورقی{Fault Tolerance}‌  در سیستم نسبت به محاسبات ابری بسیار بالا خواهدبود. از این رو در مواردی که توقف سیستم غیرقابل‌قبول باشد استفاده از محاسبات لبه مناسب خواهدبود. با توجه به مفاهیم ذکرشده و نقش آن‌ها در یک سیستم اینترنت اشیا، در راستای بهبود عملکرد سیستم وجود معماری چند لایه متشکل از محاسبات ابری، محاسبات لبه و دستگاه‌های هوشمند لازمه‌ی کارایی سیستم خواهدبود. این معماری به فراخور سیستم مورد نیاز در پژوهش‌های انجام‌شده به صورت‌های مختلفی در نظر گرفته شده‌است. به عنوان مثال در~\cite{gai2018optimal} این معماری در سه سطح محاسبات ابری، محاسبات لبه و دستگاه های هوشمند ارائه شده‌است و در آن تعریف محاسبات لبه و محاسبات مه به صورت مشترک و تحت عنوان محاسبات لبه در نظر گرفته شده است. همچنین در~\cite{li2019energy} این معماری به سه لایه محاسبات ابری، محاسبات مه و دستگاه‌های هوشمند خلاصه می‌شود.


\قسمت{بارسپاری محاسباتی}
از منظر کاربر، یک مورد استفاده حیاتی از محاسبات لبه امر بارسپاری محاسباتی است، تخلیه بار محاسباتی می‌تواند زمینه را برای مواردی مانند کاهش تأخیر و بهینه‌سازی مصرف انرژی در تجهیزات کاربر و یا کلیت سیستم، فراهم‌آورد و در نتیجه موجب بهبود کیفیت‌خدمات\پاورقی{Quality of Service} و تجریه مشتری\پاورقی{Quality of Experience} گردد. به شکل کلی، واحد جدیدی در مورد بارسپاری محاسباتی، تصمیم می‌گیرد، که آیا بارسپاری انجام‌شود یا نشود، و اینکه چقدر و چه چیزی باید بارسپاری‌شود. اساسا، تصمیم‌گیری در مورد بارسپاری محاسبات ممکن‌است منجر به مواردی از جمله موارد زیر شود~\cite{mach2017mobile}:




\شروع{فقرات}
\فقره \textbf{محاسبات محلی:}
کل محاسبات به صورت محلی در تجهیزات کاربر انجام می‌شود. به عنوان مثال، وقتی که به دلیل عدم دسترسی به منابع محاسبات لبه  و یا وجود هزینه بالا در بارسپاری محاسبات، محاسبات به شکل محلی انجام می‌شوند.


\فقره  \textbf{بارسپاری کامل:}
کل محاسبات مورد نیاز به سرورهای لبه تخلیه می‌شوند. به عنوان مثال وقتی نیاز به محاسبات زیاد وجود دارد و تأخیر ارسال قابل تحمل است. 


\فقره \textbf{‌بارسپاری جزئي:} 
در این نوع از تخلیه محاسباتی، در صورتی که محاسبات قابل تقسیم باشند، بخشی از محاسبات به صورت محلی انجام می‌شود، در حالی که مابقی به سرورهای لبه تخلیه می‌شوند. 


\پایان{فقرات}

تخلیه محاسباتی در عمل یک فرآیند بسیار پیچیده و حساس است، چرا که می‌تواند تحت تأثیر عوامل مختلفی مانند ترجیحات کاربران، قابلیت اتصال، قابلیت‌های تجهیزات کاربران و دسترسی به ابر باشد. یک جنبه مهم در تخلیه محاسبات نیز، نوع برنامه کاربردی است، زیرا تعیین می‌کند که آیا تخلیه کامل و یا پراکنده قابل اجرا است، چه چیزی و به چه شکل می تواند تخلیه شود. نحوه استفاده و مدیریت فرآیند تخلیه در عمل بسیار دشوار است. اساسا تجهیزات کاربر باید یه یک واحد تصمیم‌گیری مجهز شود که مسئولیت آن، این است که با توجه به نوع برنامه، نوع داده و هدف از تخلیه، تعیین‌کند که چه محاسباتی می‌تواند بارسپاری‌شود، و چه محاسباتی می‌بایست توسط تجهیزات کاربر انجام‌شود.


\زیرقسمت{تصمیم‌گیری در مورد بارسپاری محاسبات به گره‌ لبه}

هدف اصلی در تصمیم‌گیری تخلیه محاسباتی، معمولا به حداقل رساندن تأخیر و مصرف انرژی را شامل می‌شود. همچنین به حداقل رساندن مصرف انرژی می‌بایست در حالی که محدودیت‌های تأخیر از پیش تعریف شده است، برآورده شود. به شکل کلی اهداف اصلی از بارسپاری محاسبات را می‌توان طبق زیر طبقه‌بندی نمود. 

\پاراگراف{کمینه‌سازی تأخیر اجرا}
یکی از مزایایی که با تخلیه محاسبات به سرورهای لبه معرفی می‌شود، امکان کاهش تأخیر در اجراست. در صورتی که تجهیزات کاربر تمام محاسبات را به تنهایی انجام دهد (یعنی تخلیه بار انجام نشود)، تأخیر اجرا صرفاً نشان‌دهنده زمان صرف شده توسط اجرای محلی در  تجهیزات کاربر است، اما در صورت تخلیه محاسباتی به سرورهای لبه، تأخیر اجرا شامل سه بخش زیر خواهدبود:


\شروع{فقرات}
\فقره مدت‌زمان انتقال داده‌های تخلیه‌شده به سرورهای لبه

\فقره زمان انجام محاسبات در سرور لبه

\فقره مدت زمان بازگشت و پذیرش داده‌های پردازش‌شده 



\پایان{فقرات}


\پاراگراف{کمینه‌سازی مصرف انرژی}
یکی دیگر از اهداف در بارسپاری محاسبات این است که در حالی که محدودیت‌های زمانی مورد قبول در برنامه‌های کاربردی ارضا می‌شوند، بتوان با بهره‌گیری از سیاست‌های مناسب در تخلیه محاسبات، مصرف انرژی در تجهیزات کاربر را به حداقل رساند. بنابراین با تخلیه محاسبات به سرورهای لبه می‌توان بدون انجام محاسلبات محلی موجب کاهش مصرف انرژی در تجهیزات کاربر شد، اما از طرفی تخلیه و انتقال محاسبات به سرورهای لبه، موجب مصرف انرژی  برای انتقال محاسبات به سرورهای لبه و همچنین دریافت نتایج در تجهیزات کاربر خواهد بود.     


\قسمت{مدیریت منابع}



ادغام محاسبات در تجهیزات کاربر و منابع محاسباتی لبه، درک کامل مدیریت‌منابع\پاورقی{Resource Management} را به امری ضروری بدل می‌کند. تأخیر محاسبات و کمبود ‌منابع به‌شدت تحت‌تاثیر تراکم شبکه قرار می‌گیرند، استفاده از توان بیشتر پردازشی در محاسبات لبه، به عنوان نزدیکترین منبع محاسبات و ذخیره‌سازی، می‌تواند یک راه‌حل امیدوارکننده برای کاهش زمان تأخیر و مصرف انرژی دستگاه‌ها تلقی‌شود، و منابع غیر متمرکز نقش مهمی در به اشتراک‌گذاری این منابع‌ ایفا می‌کنند. 



همانطور که اشاره‌شد، امید می‌رود محاسبات لبه، به عنوان مکمل محاسبات ابری، مشکلات منابع محدود دستگاه‌ه‌ای هوشمند و مهلت\پاورقی{Deadlinet}وظایف حساس به تأخیر را مرتفع نماید، و البته در پی آن چالش‌های زیادی در مسیر دست‌یافتن به این امر مطرح خواهدشد. منابع محاسباتی محدود و رشد فزاینده دستگاه‌های هوشمند در محیطی متغیر و غیرقابل پیش‌بینی بر اهمیت بسیار زیاد مدیریت بهینه منابع به صورتی منعطف و مقیاس‌پذیر تأکید می‌ورزد. طبقه‌بندی مدیریت منابع، بر اساس تحقیقات فعلی در این زمینه، در~\cite{tocze2018taxonomy} ارائه شده‌است. طبق این طبقه‌بندی، در مجموع پنج دسته مختلف با هدف مدیریت منابع به شکل زیر در نظر گرفته شده‌است.


\زیرقسمت{تخمین منابع}

یکی از اولین الزامات در مدیریت منابع، توانایی تخمین میزان منابع مورد نیاز برای انجام یک وظیفه است. تخمین منابع\پاورقی{Resource Estimation}امری مهم است، به ویژه برای توانایی کنترل نوسانات تقاضا در حالی که کیفیت خوبی از خدمات را برای کاربر حفظ می‌کند. منابع موجود در گره‌های‌لبه می‌توانند متحرک باشند و بنابراین ممکن است غیرقابل دسترسی شوند، که باعث می‌شود قابلیت اطمینان به آنها نسبت به منابع موجود در یک مرکز محاسباتی کمتر شود. از طرفی تحرک کاربر نیز به این عدم اطمینان می‌افزاید. از این رو درخواست‌های مربوطه باید توسط گره‌های‌لبه کنترل‌شود. به عنوان مثال نویسندگان در~\cite{aazam2016pre} از میانگین داده‌های تاریخی برای پیش‌بینی ویژگی‌های توزیع بار و استفاده از منابع برای دوره‌های بعدی استفاده می‌کنند.

\زیرقسمت{کشف منابع}

برخلاف مسئله تخمین که مربوط به سمت تقاضای درخواست است، کشف منابع\پاورقی{Resource Discovery} مربوط به سمت عرضه منابع می‌شود. یک سیستم مدیریتی باید بداند چه منابعی برای استفاده در دسترس است، در کجا قرار دارد و چه مدت برای استفاده در دسترس خواهدبود(مخصوصاً اگر دستگاه تأمین‌کننده منبع در حال حرکت باشد). این دسته در محاسبات لبه از اهمیت بسیاری برخوردار است، زیرا هیچ منبعی همیشه تحت کنترل سیستم نیست، و بنابراین بی‌ثبات خواهد بود. در~\cite{athwani2015resource} نویسندگان الگوریتمی برای خوشه‌بندی\پاورقی{Clustering}دستگاه‌ها و کشف منابع در خوشه ارائه می‌دهند. استراتژی آن‌ها این است که هر یک از اعضای خوشه منابع خوشه خود را به سر خوشه اطلاع می‌دهد و کلیه درخواست‌ها توسط سر خوشه انجام می‌شود.


\زیرقسمت{تخصیص منابع}

دسته سوم با هدف تخصیص برنامه‌ها در نزدیکی کاربران ظاهر می‌شود. تخصیص منابع\پاورقی{Resource Allocation}، از دانش منابع موجود برای نقشه‌برداری از بخش‌هایی از برنامه‌ها در دستگاه‌های مختلف استفاده می‌کند تا نیازهای آن‌ها برآورده‌شود. چند دیدگاه مختلف از تخصیص منابع وجود دارد اینکه در چه مرحله‌ای تخصیص انجام شود، در صورت نیاز چه زمانی و چه مقدار تخصیص‌یابد. از میان رویکردهای غالب تخصیص منابع، سه دیدگاه عمده به شکل زیر تقسیم می‌شوند.

\شروع{فقرات}
\فقره \textbf{جایگذاری:} بیشتر پژوهش‌های انجام‌شده بر این دیدگاه تأکید دارند، به این شکل که وظیفه در کجا باید اجرا شود و چگونه منبعی را برای بهترین اجرای ممکن اختصاص‌داد. 

\فقره \textbf{مهاجرت:} با در نظر گرفتن اینکه تخصیص منابع باید کجا و به چه شکل انجام‌شود، صحبت از نهادهای مجازی مانند سرویس‌ها، برنامه‌ها و ماشین‌های مجازی به‌میان می‌آید، از این رو تمرکز می‌تواند بر این باشد که چگونه می‌توان آن‌ها را در حین اجرا و بر حسب نیاز برای بهبود عملکرد جابه‌جا کرد. به عنوان مثال در~\cite{chen2018task} نویسندگان با هدف کاهش تأخیر یک استراتژی مهاجرت بین منابع محاسباتی لبه در بارهای سنگین محاسباتی را ارائه می‌دهند.






\فقره \textbf{برنامه‌ریزی:} در حالی که تحقیقات گسترده‌ای در مورد زمان و تعداد منابع برای تخصیص منابع در شبکه‌ها و مناطق خاص ابر وجود دارد، برنامه‌ریزی به اولویت‌دهی و زمان‌بندی فعالیت‌های محاسباتی در منابع می‌پردازد. 

\پایان{فقرات}


\زیرقسمت{اشتراک‌گذاری منابع}

منابع موجود در دستگاه‌های انتهایی، ناهمگن و در اکثر اوقات فاقد توان محاسباتی کافی است، منابع محاسباتی‌ لبه نیز در مقایسه با منابع موجود در ابر محدودیت بیشتری دارند. اشتراک‌گذاری منابع\پاورقی{Resource Sharing} بین خود دستگاه‌ها یا بین دستگاه‌های انتهایی و گره‌های‌ لبه، با هدف تأمین منابع محاسباتی مورد نیاز از اهمیت زیادی برخوردار است. اشتراک‌گذاری منابع معمولاً با تجمیع منابع در مجاورت محلی گره‌ها محقق می‌شود. این امر می‌تواند به دامنه محاسبات لبه گسترش یابد و یا در دستگاه‌های انتهایی باقی‌بماند.


\زیرقسمت{بهینه‌سازی منابع}

هدف پنجم مورد بررسی در پژوهش‌های انجام‌شده، بهینه‌سازی منابع\پاورقی{Resource Optimization} است. این امر معمولاً یک هدف مشترک و همراه با یکی یا بیشتر از اهداف توصیف‌شده قبلی است، که به بررسی جنبه‌های مختلف در بهینه‌سازی می‌پردازد. بهینه‌سازی با ترکیب رویکردهای مدیریت منابع فوق الذکر به‌دست می‌آید. هدف اصلی بهینه‌سازی استفاده از منابع موجود در لبه با توجه به محدودیت‌های موجود در شبکه است.



\قسمت{فرایند تصمیم‌گیری مارکوف}

در یک شهود سطح بالا، فرایند تصمیم‌گیری مارکوف\پاورقی{Markov Decision Process} نوعی مدل ریاضی است برای بیان رفتار و تغییر وضعیت احتمالی محیط با توجه به تصمیمات پی‌درپی در حالات~\cite{puterman1990markov}، به شکلی که اگر به زنجیره گسسته‌زمان مارکوف\پاورقی{Discrete-time Markov Chain} عدم‌قطعیت افزوده‌شود، حاصل فرآیند تصمیم‌گیری مارکوف نامیده می‌شود که علاوه بر خواص زنجیره گسسته‌زمان مارکوف می‌توان به کمک این مدل هم رفتارهای احتمالی و هم رفتارهای دارای عدم قطعیت را مدل‌سازی نمود. 
 در یک زمان مشخص مثل $t$، عامل (تصمیم‌گیرنده)، حالت سیستم را مشاهده می‌کند. بر اساس این حالت و سیاست  تصمیم‌گیری $\pi$، عملی را انتخاب می‌کند. انتخاب این عمل دو نتیجه به همراه دارد: عامل پاداش فوری دریافت می‌کند (یا یک هزینه فوری را متحمل می‌شود) و همچنین با توجه به توزیع احتمال تعیین‌شده توسط انتخاب عمل $P$، به حالت جدیدی وارد می‌شود. در این گام از زمان، عامل مجدداً باید تصمیم‌گیری کند، اما اکنون ممکن است سیستم در حالت دیگری باشد و مجموعه اعمال دیگری را برای انتخاب پیش‌رو داشته باشد.

در واقع فرایند تصمیم‌گیری مارکوف، فرایندی جهت مدل‌سازی تصمیم‌گیری در شرایطی است که نتایج تا حدودی تصادفی و تا حدودی تحت کنترل یک تصمیم‌گیرنده است. این فرآیند برای طیف گسترده‌ای از مسائل بهینه‌سازی که از طریق برنامه‌ریزی پویا و یادگیری تقویتی حل می‌شوند، بسیار خاص و منحصربه‌فرد است. 




این مدل به عوامل هوشمند این امکان را می‌دهد تا رفتار ایده‌آل را در یک محیط خاص تعیین کنند، تا در جهت آنچه در پی آن هستند توانایی مدل را برای دستیابی به یک حالت خاص، در یک محیط به‌حداکثر برسانند. 

به طور دقیق‌تر فرایندهای تصمیم‌گیری مارکوف فرایندهای کنترل تصادفی زمان‌گسسته است و می‌توان نتیجه گرفت در فرآیند تصمیم‌گیری مارکوف، تصمیم‌گیرنده می‌تواند بر روی حالتی از سیستم با انجام عمل تاثیر بگذارد که موجب می‌شود کارایی از پیش ‌تعریف‌شده را بهینه‌کند. 


این هدف با توجه به سیاست‌ها و خواسته‌ی مورد نظر، تحت عنوان قوانینی برای رفتار سیستم تعریف می‌شود که وابسته به نوع محیط، در رفتار عامل در محیط اعمال می‌شوند. 

فرایند تصمیم‌گیری مارکوف به دنبال بهینه‌سازی اقدامات اخذشده تا دستیابی به حالت نهایی است. این بهینه‌سازی با یک سیستم بازخورد پاداش انجام می‌شود، به شکلی‌ که اقدامات مختلف بسته به حالت پیش‌بینی‌شده‌ای که این اقدامات آن را ایجاد می‌کنند، وزن‌دهی می‌شوند. 

فرآیند تصمیم‌گیری مارکوف توسط مجموعه‌ای از حالت‌ها، اقدامات، یک مدل انتقال و یک تابع پاداش تعریف می‌شود. پاداش و انتقال برطبق خواص زنجیره مارکوفی، همواره فقط به وضعیت فعلی، عمل انتخاب‌شده و وضعیت بعدی بستگی دارند، و حالت‌های قبلی اهمیتی نخواهندداشت. 

این مدل تصویری از حالت‌های متنوع محیط و اقدامات ممکن در هر حالت را تشکیل می‌دهد، که در هر وضعیت، هر عمل موجب تغییر احتمالی به وضعیت جدید خواهدبود. در هر حالت عامل یک وضعیت را مشاهده می‌کند و عملی را انجام می دهد، که باعث می‌شود پاداشی از تابع پاداش تعریف‌شده به دست آورد، و یا در سناریوی دیگری هزینه به حداقل برسد، و در همین حال، حالت جانشین فقط به وضعیت فعلی و عمل انتخابی بستگی‌دارد. همچنین ممکن است حالت جانشین، براساس عدم اطمینان ما در محیطی که جستجو در آن انجام می شود، احتمالاتی باشد. 
در ادمه به بررسی دقیق‌تر و اجزا تشکیل‌دهنده یک مدل فرایند تصمیم‌گیری مارکوف می‌پردازیم.  

\قسمت{تعریف فرایند تصمیم‌گیری مارکوف}

ازآنجا که سنگ بنای یادگیری تقویتی، فرایندهای تصمیم‌گیری مارکوف هستند؛ هر مسئله یادگیری تقویتی قابل فرموله شدن به کمک فرایند تصمیم‌گیری مارکوف است. ویژگی بارز فرایند تصمیم‌گیری مارکوف که از آن تحت عنوان "خاصیت مارکوف" یاد می‌شود این است که گذار از یک وضعیت به وضعیت بعدی تنها به وضعیت فعلی وابسته است، و نسبت به وضعیتهای قبل از آن بی‌تفاوت است که در زیر بیان ریاضی آن ذکر شده است.
$$P\Big(X_{n+1} = j \Big| X_n = i, X_{n-1} = i_{n-1}, X_{n-2} = i_{n-2}, \ldots , X_0 = i_0\Big) = P\Big(X_{n+1} = j \Big| X_n = i \Big) $$

فرایند تصمیم‌گیری مارکوف، یک مدل ریاضی احتمالی برای یک سناریوی تصمیم‌گیری است. در هر مرحله، تصمیم‌گیرنده یا همان عامل، عملی را انتخاب می‌کند که بخشی از نتایج آن تصادفی و بخشی دیگر نیز نتیجه عمل است. این فرآیندها برای مدل‌سازی انواع مسائل بهینه‌سازی به خصوص مسائلی که نیاز به تصمیم‌گیری‌های متوالی و پی‌درپی دارند مورداستفاده قرار می‌گیرند.


فرایند تصمیم‌گیری مارکوف شامل پنج عنصر است که به صورت چندتایی $M = (\mathcal{S}, \bar{s}, Act, Step, L)$ در زیر مشخص می‌شود:


\شروع{فقرات}


\فقره $\mathcal{S}$ مجموعه متناهی از حالت‌های سیستم است.

\فقره $\bar{s} \in \mathcal{S}$ حالت اولیه سیستم است. 

\فقره $Act$ مجموعه عمل‌ها در سیستم است. 

\فقره $Step: \mathcal{S} \rightarrow Dist(s)$ تابع احتمال گذار است. 

\فقره $L : \mathcal{S} \rightarrow 2 ^{AP}$ تابع برچسب‌گذاری است. برای هر حالت $s \in \mathcal{S}$، مجموعه $L(s)$ از گزاره‌های اتمی نسبت داده می‌شود. در واقع این تابع برای هر حالت مجموعه‌ای از گزاره‌های اتمی را مشخص می‌کند که در آن حالت برقرار هستند. 

\پایان{فقرات}


همانطور که مشخص است، در هر حالت ممکن است چندین عمل قابل اخذ باشد. رفتار یک فرایند تصمیم‌گیری مارکوفی به این شکل است که ابتدا در هر حالت $s \in \mathcal{S}$، از میان اعمال قابل اخذ، یک عمل مانند ${a \in Act}$ با عدم‌قطعیت انتخاب می‌شود، سپس حالت بعدی با استفاده از تابع توزیع $Step(s,a)$، مشخص می‌شود. 


\قسمت{یادگیری تقویتی}

یادگیری تقویتی\پاورقی{Reinforcement Learning} همانند روش‌های یادگیری بانظارت\پاورقی{Supervised Learning} و یادگیری بدون‌نظارت\پاورقی{Unsupervised Learning}، شاخه‌ای از علم یادگیری ماشین است که عامل طراحی‌شده برای این نوع از یادگیری بر اساس سطحی از خطا سعی در شناخت و مدل‌سازی محیط و تصمیم‌گیری بر اساس آن می‌کند. 

در این شاخه، ورودی شامل بردار ویژگی‌ها به همراه سیگنال امتیاز عمل انجام‌شده در حالت فعلی است، که عامل به کمک آن اقدام به تعیین کیفیت عمل انجام‌شده خود می نماید. همانطور که در شکل~\رجوع{شکل: عامل هوشمند یادگیری تقویتی} مشاهده می‌شود، عامل با انجام اعمال مختلف به‌ازای حالت‌های مختلف و دریافت بازخورد از محیط، اقدام به شناخت ویژگی‌های محیط نموده و از این طریق به تصمیم‌گیری می‌پردازد. 



\قسمت{اجزا مسئله یادگیری تقویتی}

در این بخش به تعریف عواملی می‌پردازیم که باعث شکل‌گرفتن و تعریف یک مسئله یادگیری تقویتی می‌گردد. یک مسئله یادگیری تقویتی استاندارد به طور کلی شامل بخش‌های زیر می‌باشد:

\زیرقسمت{سیگنال امتیاز}

یکی از بارزترین مشخصه‌های هر مسئله یادگیری تقویتی وجود سیگنال امتیاز است سیگنال امتیاز یک سیگنال عددی بازخورد\پاورقی{Feedback} است که عامل از طریق آن متوجه می‌شود با انجام دنباله‌ای از اعمال، چه میزان قابل قبول بوده‌است. عامل در هر بازه زمانی یک عمل از اعمال تعریف‌شده خود را بر طبق سیاست اتخاذ شده، انجام داده و نتیجه آن را از طریق سیگنال بازخورد دریافت می‌کنند. هدف نهایی عامل در مسئله یادگیری تقویتی بیشینه‌کردن مجموع امتیازهای به‌ دست آمده از ابتدا تا انتها است. این مهم بر اساس نظریه امتیاز شکل گرفته است که پایه و اساس روش یادگیری تقویتی است.

نظریه امتیاز به طور کلی بیان می‌کند که می‌توان اهداف را به‌صورت بیشینه کردن امتیاز کسب‌شده در طول مراحل رسیدن به هدف تعریف کرد، و مسائلی که بر پایه یادگیری تقویتی هستند همگی باید شرایط موجود در نظریه جایزه را داشته باشند تا بتوان از روش‌های مرسوم یادگیری تقویتی در آن‌ها استفاده کرد.


سیگنال امتیاز در مسائل یادگیری تقویتی عمدتا دارای تأخیر زیادی است و معمولا پس از طی قالب‌های زمانی زیادی نتیجه عمل انجام‌شده در قالب $r$ به دست می‌آید. ضمن اینکه به دلیل تاثیر مستقیم و وابسته بودن حالت‌های پیش‌آمده به یکدیگر، باید تاثیر تصمیمات فعلی بر آینده دور یا نزدیک را در نظر داشته‌باشیم که مسئله ما را به یک مسئله تصمیم‌گیری ترتیبی تبدیل می‌کند. در این مسئله هدف بیشینه‌کردن مجموع امتیاز به دست آمده ‌است. 


\زیرقسمت{عامل و محیط}

یکی دیگر از تعاریف مهم در یادگیری تقویتی تعریف عامل و محیط فعالیت است. در زیر به تعریف این دو می‌پردازیم. عامل هوشمند عاملی خودمختار است که توسط حسگرهایش محیط را درک کرده،  و بر روی محیط تغییر دلخواه را برای رسیدن به هدف تعیین‌شده، که بیشینه‌کردن امتیاز اکتسابی است، انجام می‌دهد.



\شروع{شکل}
\centerimg{rl}{12cm}
\شرح{نحوه تعامل عامل هوشمند یادگیری تقویتی در محیط}
\برچسب{شکل: عامل هوشمند یادگیری تقویتی}
\پایان{شکل}


محیط فعالیت: محیطی که عامل در آن قرارگرفته و به فعالیت در آن می‌پردازد محیط فعالیت نامیده می‌شود. جهان پیرامون عامل در یک شبیه‌ساز یک حالت داخلی دارد که حالت فعلی محیط را مشخص می‌کند که بر حسب دسترسی عامل به آن محیط به دسته‌های زیر تقسیم می‌گردد:

\شروع{فقرات}

\فقره اگر تعامل با حالت محیط دسترسی داشته باشد محیط آشکار\پاورقی{Observable} است.

\فقره اگر حالت محیط برای عامل کاملاً قابل دسترس نباشد محیط قسمتی‌آشکار\پاورقی{Partially Observable} است.

\پایان{فقرات}

یکی از وظایف محیط، دریافت عمل عامل و به‌روزرسانی حالت داخلی خود برحسب آن و محاسبه امتیاز و ارسال آن به عامل است، و این چرخه تا رسیدن به هدف تعیین‌شده ادامه می‌یابد.

به طور کلی در هر مرحله ارتباط بین عامل و محیط به نحو زیر است: 

\شروع{فقرات}

\فقره در هر بازه زمانی $t$، عامل روند زیر را طی می‌دهد:

\شروع{شمارش}

\فقره اجرای عمل $a_t$

\فقره دریافت مشاهده محیط $O_t$

\فقره دریافت امتیاز $R_t$ از محیط

\پایان{شمارش}


\فقره در هر بازه زمانی $t$، محیط روند زیر را طی می‌دهد:

\شروع{شمارش}

\فقره دریافت عمل $a_t$ از عامل

\فقره ارسال ویژگی‌های متناسب با حالت داخلی فعلی $O_{t+1}$

\فقره ارسال امتیاز عمل $R_{t+1}$ به عامل

\پایان{شمارش}


\پایان{فقرات}


\زیرقسمت{حالات و تاریخچه}

عامل برای فعالیت کارآمد نیازمند کسب اطلاعات جامعی از محیط پیرامون خود است. عامل از این اطلاعات استفاده‌کرده تا بتواند موقعیت خود را در محیط پیرامون بسنجد و بداند چه اعمالی را در گذشته انجام‌‌داده و چه تاثیراتی بر محیط داشته‌است، تا به کمک آن برای حالات آینده تصمیم‌گیری صحیح انجام‌دهد. بدین منظور نیازمند مطالعه و بررسی نحوه ذخیره‌سازی تاریخچه اعمال عامل هستیم که مفاهیم زیر به ما در این زمینه کمک می‌نمایند:

\شروع{فقرات}

\فقره $\textbf{تاریخچه}$: تاریخچه\پاورقی{History} دخیره‌شده شامل دنباله‌ای از مشاهدات، اعمال و امتیازات دریافت شده توسط عامل است.
\begin{alignat}{2}
	\textbf{H}_t = o_1,r_1,a_1, \ldots , a_{t-1}, o_t, r_t
	\label{101}  
\end{alignat}  




\فقره $\textbf{حالت}$: به دلیل پیچیدگی تاریخچه و نیاز به فضای ذخیره‌سازی بالا، نیازمند معرفی یک ساختار ساخت‌یافته‌تر برای ذخیره تاریخچه عامل هستیم که ما را به مفهوم حالت\پاورقی{State} می‌رساند. 

حالت در مفهوم کلی به معنی تابعی از تاریخچه است با این تفاوت که اطلاعات ضروری آن را استخراج کرده و از آن‌ها برای  تشخیص موقعیت خود و تصمیم‌گیری استفاده می‌کنند، بدون آنکه درگیر پردازش و ذخیره سازی اطلاعات غیرضروری شود.
\begin{alignat}{2}
	\mathcal{S}_t = f(\textbf{H}_t)
	\label{102}  
\end{alignat}  


به طور کلی حالت‌های تعریف‌شده در مسائل یادگیری تقویتی شامل دو دسته کلی زیر است:

\شروع{شمارش}

\فقره \textbf{حالت درونی:}\پاورقی{Internal State} در داخل عامل ذخیره می‌شود و عامل به کمک آن وضعیت فعلی خویش را درک می‌کند. 

\فقره \textbf{حالت برونی:}\پاورقی{Environment State} یک خصیصه در محیط فعالیت عامل در صورت استفاده از شبیه‌ساز است و وضعیت فعلی محیط را مشخص می‌کند، و از طریق یک بازنمایی از آن عامل می‌تواند درک مناسبی از جهان پیرامون خود به‌دست‌آورد و معمولاً حالت داخلی محیط از دید عامل مخفی است.


\پایان{شمارش}

یکی از مهم‌ترین خصیصه‌های حالت در مسائل یادگیری تقویتی، دارا بودن خاصیت مارکوف درجه اول است. بدین معنی که تنها با داشتن حالت فعلی بتوان اطلاعات لازم برای درک وضعیت فعلی محیط به‌دست‌آورد و نیازی به حالت پیشین نباشد. به حالتی که این شرط را ارضا نماید حالت اطلاعاتی\پاورقی{Information State} ‌گوییم. طراحی حالت اطلاعاتی که خاصیت مارکوف درجه اول را ارضا کند نیازمند برقرار بودن رابطه ۲.۴ است:
\begin{alignat}{2}
	P[\mathcal{S}_{t+1}|\mathcal{S}_t] = P[\mathcal{S}_{t+1}|\mathcal{S}_{1}, \ldots, \mathcal{S}_t]
	\label{103}  
\end{alignat}  


\پایان{فقرات}
 با دانستن حالت دیگر نیازی به نگهداری تاریخچه نیست و می توان تاریخچه را کنار گذاشت. به همین دلیل تعریف صحیح حالت تاثیر مهمی در شناسایی محیط توسط تعامل دارد و می‌توان با مشاهده یکسان ولی با نحوه متفاوت طراحی حالت ها عامل را به انجام اعمال متفاوت و گاهی اشتباه وادار نمود. به این دلیل طراحی حالت صحیح، یکی از مهمترین اجزاء تعریف مسئله یادگیری تقویتی است.


\زیرقسمت{توابع ارزش و معادله بلمن}

تقریبا تمامی الگوریتم‌های یادگری تقویتی به دنبال تخمین توابع ارزش هستند. این توابع می‌خواهند میزان خوب بودن یک حالت برای بودن عامل در آن که به آن تابع ارزش حالت می‌گوییم، یا میزان خوب بودن انجام عمل $a$ در یک حالت که به آن تابع ارزش عمل می‌گوییم را، تخمین بزنند.

از این رو در این بخش قبل از معرفی سیاست و سیاست بهینه باید این دو مفهوم اصلی را تعریف کنیم. تابع ارزش حالت و تابع ارزش اعمال در حالت، توابعی هستند که ما با کمک آنها سیاست بهینه را استخراج می‌کنیم. حال یک سیاست فرضی مثل $\pi$ را در نظر بگیرید، طبق تعریف ارزش حالت $s$ تحت سیاست $\pi$ برابر است با:
\begin{alignat}{2}
	V_\pi(s) = E_\pi \big[G_t | s_t = s \big] = E_\pi \bigg[ \sum\limits_{k=0}^\infty \gamma^{k} r_{t+k+1} \big| s_t = s \bigg]
	\label{1000}  
\end{alignat}  


که به معنای امید ریاضی پاداشهای دریافتی تا به حال با شروع از حالت $s$ و طی کردن مسیرها تحت سیاست $\pi$ به دست آورده است. به رابطه با معادله بلمن برای $V_\pi(s)$ نیز می‌گویند، که رابطه بین ارزش یک حالت با ارزش حالت‌های بعدی را بیان می‌کند. در فصل‌های بعدی به دنبال تخمین این تابع ارزش با کمک الگوریتم‌های یادگیری تقویتی هستیم. مفهوم دیگری تحت عنوان تابع ارزش اعمال نیز به صورت زیر تعریف می‌شود:
\begin{alignat}{2}
	Q_\pi(s,a) = E_\pi \big[R_t | s_t = s, a_t = a \big] = E_\pi \bigg[ \sum\limits_{k=0}^\infty \gamma^{k} r_{t+k+1} \big| s_t = s, a_t = a \bigg]
	\label{1001}  
\end{alignat}  


که به معنای امید ریاضی پاداشهای دریافتی تا به حال با شروع از حالت $s$ و انجام عمل $a$ و طی کردن مسیرها تحت سیاست $\pi$ است. حال می‌توانیم از این تعاریف برای تشکیل معادله بلمن استفاده کنیم. لازم به ذکر است که این تبدیل در []، در دسترس است و در این قسمت صرفا فرم نهایی معادله بلمن را نمایش می‌دهیم. معادله بلمن برای ارزش حالت و ارزش اعمال در حالت به صورت زیر نوشته می‌شود:
\begin{alignat}{2}
	V_\pi(s) = \sum\limits_a \pi(s,a) \sum\limits_{s\prime}  P(s,a,s\prime) \big[ R(s,a,s\prime) + \gamma V_\pi(s\prime)\big]
	\label{1002}  
\end{alignat}  
\begin{alignat}{2}
	Q_\pi(s,a) = \sum\limits_{s^\prime} P(s,a,s^\prime) \bigg[ R(s,a,s^\prime) + \gamma \sum\limits_{a^\prime} \pi (s^\prime , a^\prime) Q(s^\prime, a^\prime) \bigg]
	\label{1003}  
\end{alignat}  


\قسمت{بهره‌برداری و اکتشاف}
یکی از مسائل مهمی که همیشه در یادگیری تقویتی با آن مواجه بوده‌ایم مسئله سبک‌سنگین کردن\پاورقی{Trade-off} بین بهره‌برداری\پاورقی{Exploitation}  و جستجو\پاورقی{Exploration} است. در ابتدا به تعریف این دو مفهوم می‌پردازیم:


\شروع{فقرات}

\فقره $\textbf{بهره برداری:}$
در بهره‌برداری هدف اصلی بیشینه‌کردن سود ما به ازای هر انتخاب است، به نوعی می‌خواهیم با استفاده از دانش کسب‌شده، حداکثر بهره ممکن را به‌دست‌آوریم. پس در هر زمانی که نیاز به تصمیم‌گیری توسط عامل احساس می شود، در صورتی که عامل هدف اصلی‌اش بهره‌برداری باشد باید عملی را انجام دهد که امتیاز وی را بیشینه نماید.

\فقره $\textbf{اکتشاف:}$ 
در روش اکتشاف هدف اصلی کشف حالات جدید است. در هنگامی که عامل اقدام به کاوش در محیط می‌نماید، بسیاری از محیط‌ها برایش ناشناخته هستند و تاکنون آن‌ها را مشاهده نکرده است. در حالت جست و جو، هدف اصلی مشاهده بیشترین حالت ناشناخته ممکن است، پس باید عملی را انتخاب کنیم که ما را به حالتی جدید ببرد.
\پایان{فقرات}

در مسائل یادگیری تقویتی هر دو روش بهره‌برداری و اکتشاف را باید به صورت همزمان استفاده‌کرد، تا بتوانیم به جواب بهینه برسیم. بهره برداری را به این دلیل نیاز داریم که امتیاز خود را بیشینه کنیم و به این دلیل از جستجو بهره می گیریم تا حالت های جدید را به امید اینکه از حالت‌های فعلی امتیاز بیشتری دارند، مشاهده و کشف نماییم. باید بین این دو روش تناسبی برقرار نمود. اصولاً در ابتدا میزان جستجو زیاد است و با مشاهده حالت‌های بیشتر میزان بهره‌وری را زیاد می‌کنیم. با پیگیری این روش می‌توان به جواب بهینه مناسبی دست پیدا‌کرد.

یکی از ویژگی‌های کلیدی یادگیری تقویتی چالش وزن‌دهی به دو عامل بهره برداری و اکتشاف است. اگر عامل بخواهد اعمال بهتری را بیاموزد یا به عبارتی دیگر اعمالی که در نهایت به پاداش انباشته بیشتری منجر شود، باید اعمال جدید را به‌صورت غیر حریصانه امتحان کند. همچنین اگر عامل از دانش فعلی خود بهره برده و اقدامات قبلاً شناخته‌شده برای بازخورد پاداش‌های خوب را دنبال کرده‌باشد، تضمین نمی‌شود که بازدهی بالاتر از پاداشی که عامل می‌تواند دریافت کند داشته باشد.

بنابراین این موضوعی است که یک عامل هنگام تصمیم‌گیری برای اعمال بعدی با آن روبرو می‌شود. یا اعمال تصادفی را امتحان‌کند و حدس بزند که پاداش بیشتری می‌گیرد اما با خطر بدتر شدن پاداش مواجه است، یا اینکه با عملکرد مطابق با شرایط فعلی خود به پاداش احتمالاً پایین‌تر اما مطمئن دست پیدا کند. به‌عبارت دیگر اگر عامل تنها اکتشاف را انجام دهد ممکن است در این کار به امتیازات بالاتری نرسد و اعمال خود را بهبود ندهد. در طرف دیگر اگر فقط از استخراج استفاده شود ممکن است در خط‌مشی فعلی خود با دیدن تمام خط سیرهای احتمالی گیر کند. از همین رو عامل احتمالاً خطمشی بهینه را از دست خواهد داد. بنابراین باید یک تعادل مناسب بین اکتشاف و بهره برداری وجود داشته باشد.

این معضل از آنجایی ناشی می‌شود که معمرلا فرایند یادگیری در یادگیری تقویتی به‌ صورت برخط صورت می‌گیرد. به‌عبارت دیگر به یادگیری تقویتی هیچ داده‌ای همانند یادگیری با نظارت داده نمی‌شود بنابراین عامل خود به‌ نوعی به دنبال جمع‌آوری داده‌ها است و از طریق اعمالی که انجام می‌دهد بر داده‌های مشاهده شده اثر می‌گذارد، و از همین رو گاهی ارزش دارد که اعمال مختلفی را برای به‌دست آوردن داده‌های جدید انجام‌دهد.
 
از این رو برای ایجاد توازن جهت یادگیری شبکه‌های عصبی، می توان از استراتژی انتخاب حریصانه-اپسیلون استفاده کرد، که روشی ساده و در عین حال کاربردی برای انتخاب
عمل در هر مرحله به‌عنوان استراتژی انتخاب، انتخاب حریصانه-اپسیلون است. در این روش یک پارامتر بین صفر تا یک انتخاب می‌شود که اقدام عامل مبنی بر اینکه بهره برداری یا اکتشاف انجام دهد را کنترل می‌کند. با استفاده از این روش در هر زمان عامل به طور احتمالی بین بهره برداری و استخراج یکی را انتخاب می‌کند و با احتمال ($\epsilon$) انتخاب تصادفی از بین تمام اعمال موجود و با احتمال ($1-\epsilon$) بهره برداری را انجام می‌دهد. و مقدار ( $\epsilon$) در طی مراحل یادگیری همواره کاهش می‌یابد. عبارت زیر این امر را تعریف می‌نماید.


\قسمت{روش‌های پاسخ به مسائل یادگیری تقویتی}

در این قسمت می‌خواهیم به بررسی روش‌هایی که برای حل مسائل یادگیری تقویتی به‌کار می‌روند بپردازیم. روش‌های مختلفی برای این منظور ارائه شده‌است که به تفصیل به توضیح آنها پرداخته و نقاط ضعف و قوت آن‌ها را بررسی می‌کنیم.

\زیرقسمت{روش تکرارشونده ارزش} 

در این روش نیازی به داشتن $\Pi$ که نمایانگر سیاست تعامل در مواجهه با حالت‌های مختلف است، نیست و به جای آن مقدار $\Pi(s)$ در هر جا که نیاز باشد، از روی $V(s)$ به صورت زیر محاسبه می‌شود.
\begin{alignat}{2}
	V_{i+1}(s) = max_a \bigg[ \sum\limits_{s^{'}} P_a(s,s^{'})[R(s,a,s^{'})+\gamma V_i(s)] \bigg]
	\label{103}  
\end{alignat}  

مقدار $i$ نمایش‌دهنده تکرار $i$ام است و مقدار اولیه $V_i$ که بیانگر نقطه شروع اعمال روش است، را به صورت تخمینی از مقادیر ممکن تعیین می‌کنیم. با اجرای الگوریتم و محاسبه مقدار $V_i$ قدم به قدم به جواب اصلی همگرا می‌شویم. 


\زیرقسمت{روش تکرارشونده سیاست}

این روش با یک سیاست تصادفی آغاز می‌شود و عامل در هر دور اجرای الگوریتم سعی در بهینه‌تر کردن سیاست اتخاذ‌شده می‌کند. این روش از برنامه‌نویسی پویا \پاورقی{Dynamic Programming} استفاده می‌کند و هدف اصلی آن به‌دست‌آوردن سیاست بهینه است. روش تکرارشونده سیاست از دو مرحله کلی زیر تشکیل شده است:


\شروع{شمارش}


\فقره \textbf{ارزیابی سیاست:}\پاورقی{Policy Evaluation} 

 در این مرحله عامل سعی می‌کند به ازای سیاست داده‌شده مقدار $V_\Pi(s)$ که نمایانگر تخمین امتیاز به‌دست‌آمده با پیگیری سیاست $\pi$ است، محاسبه و به‌روز‌رسانی نماید.


\فقره \textbf{بهبود سیاست:}\پاورقی{Policy Improvement} 

در این مرحله عامل سعی می‌کند با توجه به مقدار $V_\Pi(s)$ محاسبه شده در مرحله قبل، سیاست جدید را به‌روزرسانی کند.

\پایان{شمارش}

با استفاده از روش‌های توضیح داده‌شده، به راحتی می‌توان سیاست بهینه را به‌دست آورد و بر اساس آن امتیاز عامل را بیشینه نمود. 

مشکل اصلی روش‌های بیان‌شده، نیاز به ذخیره مقدار امتیاز هر حالت در یک جدول\پاورقی{Look Up Table} برای مشاهده مقادیر از روی آن است که در مسائل با تعداد حالت‌های بالا عملاً به دلیل نیاز به حافظه فراوان غیرقابل استفاده است. بدین منظور از روش‌های تقریب تابع\پاورقی{Function Approximation} استفاده می‌کنیم.


\قسمت{تقریب توابع}

همانطور که پیش‌تر اشاره‌شد یکی از مشکلات اصلی روش‌های مبتنی بر برنامه نویسی پویا نیاز فراوان آن‌ها به حافظه است، که عملاً استفاده از آن‌ها را برای مسائل پیچیده منتفی می‌نماید~\cite{sutton2018reinforcement}. به همین دلیل از روش‌های دیگری برای محاسبه امتیاز استفاده می‌شود. 

این روش‌ها سعی در تقریب تابع‌ای دارند که حالت فعلی را به صورت ورودی دریافت‌کرده و امتیاز آن را محاسبه می‌نماید. این روش‌ها توانایی خوبی در زمینه تشخیص حالت‌های نزدیک به یکدیگر و گرفتن تصمیم مناسب در این حالت‌ها ندارند. در حالت کلی دو نوع تابع زیر را برای تصمیم‌گیری استفاده می‌کنیم:

\شروع{شمارش}


\فقره \textbf{تابع ارزش:}\پاورقی{Value Function}
این تابع حالت فعلی را به عنوان ورودی گرفته، و با تخمین امتیاز کسب‌شده از حالت فعلی تا حالت نهایی، با رفتن به حالت جدید امکان تصمیم‌گیری برای اینکه کدام‌یک از حالت‌های ممکن بهترین حالت برای ادامه مسیر هستند را برای ما ممکن می‌کند.

\فقره \textbf{تابع امتیاز-عمل:}\پاورقی{Action-value Function}
این تابع با دریافت حالت فعلی و عمل دلخواه، امتیازت تخمینی در صورت انجام عمل ورودی و مسیر از حالت فعلی به حالت نهایی را محاسبه می‌کند، و می توان بر اساس آن برای پیدا کردن سیاست بهینه تصمیم‌گیری نمود.

\پایان{شمارش}

این دو تابع، توابع اصلی در روش یادگیری تقویتی هستند، که ما سعی در تخمین آن‌ها برای تصمیم‌گیری صحیح داریم. روش‌های متفاوتی برای تخمین توابع وجود دارد که یکی از بهترین روش‌های فعلی برای تخمین توابع استفاده از شبکه‌های عصبی عمیق\پاورقی{Deep Neural Networks} است.


\قسمت{شبکه‌های عصبی عمیق}

شبکه‌های عصبی، سیستم‌ها و روش‌های نوین محاسباتی هستند، که در تخمین پاسخ‌های مسئله‌های پیچیده در یادگیری ماشین استفاده می‌شوند~\cite{lecun2015deep}. این شبکه‌ها از ترکیب واحدهای محاسباتی کوچکی به نام نورون\پاورقی{Neuron} با آرایش‌های خاصی همانطور که در شکل~\رجوع{شکل: شبکه عصبی برگرفته} مشاهده می‌شود، ساخته شده‌اند که توانایی حل دامنه وسیعی از مسائل را دارا می‌باشند. مشکل اصلی شبکه‌های عصبی کم‌عمق\پاورقی{Shallow} این است که این شبکه‌ها، تخمین‌گر خوبی در فضای محلی داده‌های ورودی هستند. یک تصمیم محلی تنها در صورتی می‌تواند برای داده‌های جدید ورودی به خوبی تصمیم‌گیری نماید که داده‌های آموزشی در همسایگی $X$ بتوانند به خوبی مقدار تابع در $X$ را توصیف نمایند. در شکل ۲-۲ یک شبکه عصبی در حالت کلی نشان داده شده‌است. 



\شروع{شکل}
\centerimg{network}{13cm}
\شرح{نمونه‌ای از یک شبکه عصبی}
\برچسب{شکل: شبکه عصبی برگرفته}
\پایان{شکل}



شبکه های عصبی کم‌عمق توانایی افراز فضای ورودی به ناحیه‌های مختلفی را دارند، و می‌تواند مشخصات و پارامترهای لازم برای توصیف هر کدام از این نواحی را به‌دست‌آورند. وقتی تابع پیچیده باشد تعداد نواحی زیاد شده و به همین دلیل شبکه نیاز به تخمین پارامترهای بسیاری دارد. برای یادگیری و تنظیم این پارامترها نیازمند تعداد بسیار زیادی داده آموزشی هستیم، که در عمل در بسیاری از مسائل به تعداد مورد نیاز، داده آموزشی در اختیار نداریم. به همین دلیل توانایی شبکه‌های عصبی کم‌عمق در حل مسائل پیچیده به شدت کاهش می‌یابد و برای حل مسائل پیچیده به استفاده از شبکه‌های عصبی عمیق روی می‌آوریم~\cite{schmidhuber2015deep}. 

شبکه عصبی عمیق دارای تعریف روشنی نیست و اصولاً به شبکه‌ای گفته می‌شود که دارای تعداد لایه‌های بیش از سه عدد است و در اتصالات بین این لایه‌ها از توابع غیر خطی استفاده شود. این مهم به ما این توانایی را می‌دهد که بتوانیم مسائل بسیار پیچیده را تحلیل کرده و پاسخ مناسب آن‌ها را پیش‌بینی کنیم، به همین دلیل مسائل بسیاری را می‌توان ذیل شبکه‌های عصبی عمیق تحلیل نمود.


استفاده از شبکه‌های عصبی عمیق در سال‌های اخیر فراگیر گشته‌است. در سال‌های پیشین با توجه به قدرت محدود محاسباتی و عدم وجود روش‌های بهینه‌سازی کارا، شبکه های عصبی عمیق توانایی محاسبه پاسخ مناسب در بسیاری از مسائل را نداشتند. علیرغم وجود روش بازنشر خطا\پاورقی{Error Back Propagation} برای یادگیری مولفه‌های شبکه‌های چند لایه همچنان لایه‌های شبکه نمی‌تواند از تعداد مشخصی بیشتر شود. مشکل اصلی در الگوریتم پس‌انتشار خطا و نحوه بهینه‌سازی است، که وقتی تعداد لایه‌های شبکه از حدی بیشتر می‌شود، در عمل توانایی خود را برای پیدا کردن بهینه مناسب از دست می‌دهد~\cite{schmidhuber2015deep}.

یادگیری تقویتی عمیق به استفاده از شبکه‌های عصبی عمیق به عنوان تقریب توابع در تابع مقدار یا خط‌مشی در چهارچوب یادگیری تقویتی اشاره دارد و در روش‌های گرادیان سیاست\پاورقی{Policy Gradient}، یادگیری $Q$ و عامل-منتقد\پاورقی{Actor-Critic} با موفقیت اعمال شده است. در ادامه به تشریح الگوریتم‌های مدرن یادگیری تقویتی عمیق که در یادگیری تقویتی عادی نیز کاربرد دارند اشاره خواهد شد.

در ابتدا نیاز است تا توضیحاتی راجع به مفهوم شبکه‌های عصبی و ساختار آن ارائه شود، شبکه‌های عصبی مصنوعی یک شبیه‌سازی از ساختار مغز انسان است و بر پایه این فرضیات می‌باشد این شبکه‌های عصبی از تعداد زیادی واحد ساده به نام نورون تشکیل شده است:
\شروع{فقرات}
\فقره پردازش اطلاعات در ساختارهایی ساده با تعداد زیاد به نام نورون‌ها انجام می‌گیرد.

\فقره سیگنال‌ها از طریق اتصالات بین نورون‌های شبکه منتقل می‌شوند.

\فقره هر اتصال وزن مربوط به خود را دارد که در یک شبکه عصبی این وزن‌ها در سیگنال انتقالی ضرب
می‌شوند.

\فقره هر نورون یک تابع فعال‌سازی\پاورقی{Activation Function} را بر ورودی‌های خود که جمع وزن‌دار سیگنال‌های ورودی می باشد، 
اعمال کرده تا سیگنال خروجی را ایجاد نماید.

\پایان{فقرات}


\قسمت{مدل محیط در روش‌های مبتنی بر یادگیری تقویتی}

همان‌طور که پیش‌تر گفته شد، هدف اصلی عامل در یادگیری تقویتی جمع‌آوری بیش‌ترین میزان پاداش در بلندمدت است. در جهت انجام این کار عامل باید یک خط‌مشی بهینه برای رفتار در محیط پیدا کند. حال این محیط می‌تواند قطعی و یا تصادفی باشد. به همین منظور در یادگیری تقویتی مسائل اغلب از منظر ریاضی به عنوان یک فرایند تصمیم‌گیری مارکوف بیان می‌شوند، چرا که این فرایندها روشی برای نمایش پویایی محیط هستند. تابع پاداش باتوجه‌ به وضعیت فعلی عامل در محیط و عملی که توسط آن در محیط انجام می‌شود، تعریف می‌شود. توابع پاداش و انتقال اغلب "الگوی محیط" نامیده می‌شوند. با این حال گاهی اوقات توابع پاداش و انتقال در دسترس نخواهد بود، از همین رو نمی‌توانیم خط‌مشی را تخمین بزنیم، زیرا ناشناخته است. در غیاب این توابع، در جهت تخمین خط‌مشی بهینه نیاز به تعامل با محیط و مشاهده پاسخ‌های آن است که اغلب به‌ عنوان مشکل
یادگیری تقویتی از آن یاد می‌شود.

با گذشت زمان عامل شروع به درک نحوه واکنش محیط به اقدامات خود می‌کند و می‌تواند خط‌مشی بهینه را تخمین بزند. بنابراین در مسائل یادگیری تقویتی عامل خط‌مشی بهینه را برای رفتار در یک محیط ناشناخته با تعامل با آن با استفاده از روش "آزمون و خطا" تخمین می‌زند. براین اساس الگوریتم‌های یادگیری تقویتی را می‌توان به دو دسته کلی الگوریتم‌های مبتنی بر مدل و یا بدون مدل تقسیم‌بندی کرد:


\شروع{فقرات}
\فقره \textbf{روش‌های مبتنی بر مدل:}
در الگوریتم‌های مبتنی بر مدل عامل به یک مدل کامل از محیط دسترسی
دارد یا سعی می‌کند از طریق تعامل بیاموزد و به‌منظور برآورد درست خط‌مشی بهینه از تابع انتقال و  پاداش استفاده می‌کند. عامل می‌داند که باتوجه به وضعیت فعلی و عمل موردنظر چقدر احتمال دارد به یک وضعیت خاص وارد شود. با اینحال باید توجه داشت که توابع انتقال و پاداشی که عامل برای بهبود تخمین خط‌مشی بهینه خود استفاده می‌کند ممکن است فقط تقریبی از توابع واقعی باشد. از این رو خط‌مشی بهینه به دلیل این تقریب‌ها ممکن است هرگز یافت نشود.

\فقره  \textbf{روش‌های بدون مدل:}
در مقابل الگوریتم‌های مبتنی بر مدل، الگوریتم‌های بدون مدل هیچ دانش
اولیه در مورد تابع انتقال ندارند و باید ضمن یادگیری در یافتن مسیرهای کارآمد آن را بیاموزند.  به‌عبارت دیگر یک الگوریتم بدون مدل "تابع مقدار" و یا "خطمشی" را مستقیماً از طریق تجربه یعنی
با تعامل بین عامل و محیط تخمین می‌زند، بدون اینکه از توابع انتقال و پاداش اطلاعی داشته باشد.

\پایان{فقرات}

هر دو روش دارای نقاط قوت و ضعف هستند. روش‌های بدون مدل تا حدودی تضمین می‌کنند که در نهایت خط‌مشی بهینه را پیدا می‌کنند. با اینحال آنها از داده‌ها در طول آزمایشات بسیار ناکارآمد استفاده می‌کنند و بنابراین اغلب برای دست‌یابی به عملکرد خوب به تجربه زیادی نیاز دارند. در مقابل الگوریتم‌های مبتنی بر مدل می‌توانند بر این مشکل فائق بیایند، اما عامل تنها برای یک مدل خاص یاد می‌گیرد و گاهی اوقات برای برخی از مدل‌های دیگر مناسب نیست. 


\قسمت{جمع‌بندی و نتیجه‌گیری}


با مطالعه ادبیات موجود در زمینه پژوهشی مورد مطالعه به ارائه روشی نوین جهت تخصیص بهینه منابع در یک محیط محاسباتی لبه می‌پردازیم.  با توجه به این که توسعه دستگاه‌های هوشمند متحرک و بهبود ارتباطات و قابلیت‌های ادراکی جدید موجب تکثیر بسیاری از برنامه‌های کاربردی پیچیده و محاسباتی شده‌است. از این رو دستگاه‌های هوشمند با منابع محدود بیش از هر زمان دیگری با محدودیت‌های شدید ظرفیت روبه‌رو هستند. بنابراین می‌توان سیر رشد سریع تحقیقات در این حوزه را شاهد بود. 

پردازش وظایف محاسباتی تولید شده هر روزه افزایش خواهند یافت و نیاز به روش‌هایی هوشمند و پویا در مدیریت سیستم بسیار اهمیت خواهد داشت. با مطالعه پژوهش‌های انجام‌شده می‌توان دریافت که درنظرگرفتن شرایط دنیای واقعی اعم از پویایی اجتناب ناپذیر نیازهای برنامه‌های کاربردی، باعث ازکارافتادگی روش‌های پیشین در مواجهه با مسائل تخصیص منابع شده است. از طرفی عملکرد درخشان یادگیری ماشین و به‌ ویژه یادگیری تقویتی در مسائل دنیای واقعی، ما را به سمت انجام پژوهشی در این زمینه سوق داد.

 دنیای امروز، دنیای عدم قطعیت در تمامی ابعاد است. در پژوهش پیش رو سعی شده‌است تا یکی از انواع عدم قطعیت در مسائل تخصیص منابع را شبیه‌سازی کرده و به کمک یادگیری تقویتی یک رویکرد کارا برای حل آن ارائه کنیم.



%در سال ۲۰۰۶ یک روش جدید به نام شبکه باور عمیق\پاورقی{Deep Belief Networks} معرفی‌شد. هدف اصلی این روش بدون ناظر پیدا‌کردن یک نقطه مناسب در فضای پارامتری شبکه عصبی عمیق، برای شروع کار الگوریتم پس‌انتشار خطا بود. هنگامی که مقادیر اولیه پارامترها برابر مقدار به‌دست‌آمده توسط این روش نوین تنظیم می‌گشت الگوریتم پس‌انتشار خطا به دلیل اینکه در نقطه شروع نسبتاً مناسبی قرار می‌گرفت می‌توانست به سرعت به مقادیر مناسبی از مولفه‌ها همگرا شود. به این وزن‌دهی اولیه در ادبیات شبکه‌های عصبی عمیق پیش‌آموزش\پاورقی{PreTraining} گفته می‌شود~\cite{lecun2015deep}.


%روش شبکه‌های باور عمیق ~\cite{hinton2009deep} به عنوان یک نقطه شروع برای استفاده گسترده از شبکه‌های عصبی عمیق است که به استفاده از آن‌ها کمک شایانی نمود. عوامل موثر دیگری در طول زمان باعث استفاده بیش از پیش از شبکه‌های عصبی عمیق شدن که از جمله آن‌ها می‌توان به تولید مجموعه داده‌های بزرگ پیدایش ابزارهای محاسبات مناسب، بهبود در الگوریتم های بهینه‌سازی و آموزش و پیشرفت قدرت پردازش نیز اشاره نمود.

