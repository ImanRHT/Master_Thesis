
% -------------------------------------------------------
%  English Abstract
% -------------------------------------------------------


\pagestyle{empty}

\begin{latin}

\begin{center}
\textbf{Abstract}
\end{center}
\baselineskip=.8\baselineskip

With the explosion of mobile smart devices, many computation intensive applications have emerged, such as interactive gaming and augmented reality. Mobile edge computing (EC) is put forward, as an extension of cloud computing, to meet the low-latency requirements of the applications. In mobile edge computing systems, an edge node may have a high load when a large number of mobile devices offload their tasks to it. those offloaded tasks may experience large processing delay or even be dropped when their deadlines expire. Due to the uncertain load dynamics at the edge nodes, it is challenging for each device to determine its offloading decision (i.e., whether to offload or not, and which edge node it should offload its task to) in a decentralized manner. In this work, we studied the computational task offloading problem with non-divisible and delay-sensitive tasks in the MEC system, and formulate a task offloading problem to minimize the expected long-term cost. We propose a model-free deep reinforcement learning-based distributed algorithm, where each device can determine its offloading decision without knowing the task models and offloading decision of other devices. To improve the estimation of the long-term cost in the algorithm, we incorporate the long short-term memory (LSTM), dueling deep Q-network (DQN), and double-DQN techniques. Simulation results show that our proposed algorithm can better exploit the processing capacities of the edge nodes and significantly reduce the ratio of dropped tasks, average delay and energy consumptions in entire system, when compared with several basic method.

\bigskip\noindent\textbf{Keywords}:
Edge Computing, Computation Offloading, Resource Allocation, Energy Consumptions, Deep Reinforcement Learning.


\end{latin}

\newpage
